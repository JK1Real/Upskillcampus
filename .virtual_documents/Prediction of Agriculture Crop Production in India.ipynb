import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt


produces = pd.read_csv("produce.csv")

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)


produces.shape


produces.columns


produces = produces[['Particulars', 'Frequency', 'Unit',  ' 3-2005', ' 3-2006', ' 3-2007',
       ' 3-2008', ' 3-2009', ' 3-2010', ' 3-2011', ' 3-2012', ' 3-2013',
       ' 3-2014']]


#produce.iloc[1,:] # ows for year column


#produce.iloc[0,:].index[:3] # columns


produces[produces['Particulars'].str.contains('Area')].loc[108:].shape


produce = produces[produces['Particulars'].str.contains('Area')].loc[108:]


produce.shape


Particulars = produce.iloc[0,:].values[0]
Frequency = produce.iloc[0,:].values[1]
Unit = produce.iloc[0,:].values[2]
year =  produce.iloc[0,:].index[3:].tolist()
volume = produce.iloc[0,:].values[3:].tolist()


data = pd.DataFrame()
for i in range(0,80):
    
    Particulars = produce.iloc[i,:].values[0]
    Frequency = produce.iloc[i,:].values[1]
    Unit = produce.iloc[i,:].values[2]
    year =  produce.iloc[i,:].index[3:].tolist()
    Area = produce.iloc[i,:].values[3:].tolist()
    
    
    df = pd.DataFrame({'Particulars':Particulars,'Frequency':Frequency,'Unit':Unit,'year':year,'Area':Area})
    data = pd.concat([data,df],axis=0,ignore_index=True)


data['State'] = data['Particulars'].apply(lambda x:x.split('Area')[-1].strip())


data = data.loc[70:]


data.reset_index(inplace=True)



#data = data.drop(columns=['level_0','index'])


data['crop'] = data['Particulars'].str.split(' ').apply(lambda x:x[3])


data = data.loc[190:].reset_index(drop=True)


data = data.drop(columns='Particulars')


data.shape


produces[produces['Particulars'].str.contains('Volume')].loc[198:].shape


produce1 = produces[produces['Particulars'].str.contains('Volume')].loc[198:]


produce1.head()





data1 = pd.DataFrame()
for i in range(0,107):
    
    Particulars = produce1.iloc[i,:].values[0]
    Frequency = produce1.iloc[i,:].values[1]
    Unit = produce1.iloc[i,:].values[2]
    year =  produce1.iloc[i,:].index[3:].tolist()
    Volume = produce1.iloc[i,:].values[3:].tolist()
    
    
    df1 = pd.DataFrame({'Particulars':Particulars,'Frequency':Frequency,'Unit':Unit,'year':year,'Volume':Volume})
    data1 = pd.concat([data1,df1],axis=0,ignore_index=True)


data1.head()





data1['State'] = data1['Particulars'].apply(lambda x:x.split('Volume')[-1].strip())


data1.head()





data1['crop'] = data1['Particulars'].str.split(' ').apply(lambda x:x[3])


data1 = data1.loc[190:].reset_index(drop=True)


data1 = data1.drop(columns='Particulars')


data1.shape


data1['State'].unique()


data['State'].unique()


data['State'] = data['State'].str.replace(' ', '')


data1['State'] = data1['State'].str.replace(' ', '')


data['State'].unique()


data1['Unit'].unique()


data1.drop(data1[data1['Unit'] == 'Ton mn'].index, inplace=True)


data1.head()


data.drop(columns='Unit',inplace=True)


data.head()


data.reset_index(inplace=True)


data1.columns


data.columns


data['crop'].unique()


data['crop'].unique()


data1['crop'].shape


data1 = data1.dropna(subset=['Volume'])


data1.isnull().sum()


data.isnull().sum()


data = data.dropna(subset=['Area'])


data.isnull().sum()


data.shape


data1.shape


data = data.drop(columns=['level_0','index'])


data.shape


data['State'].unique()


data1['State'].unique()


data['year'].unique()


final_data = pd.merge(data, data1, on=['State', 'crop', 'year'],how='inner')


final_data.drop('Frequency_y',axis=1,inplace=True)


final_data.rename(columns={'Frequency_x': 'Frequency'}, inplace=True)


final_data.head()


final_data.to_csv('Final_data.csv', index=False)


final_data = pd.read_csv('Final_data.csv')


final_data.head()


#unit for volume above is ton th that is ton in thousands lets convert to million tons
final_data['Unit'] = 'Ton mn'
final_data['Volume'] = final_data['Volume'].apply(lambda x: x/1000)


final_data['year'].unique()


final_data.head()


## checking if there are any missing values in the dataset.
final_data.isnull().sum()


final_data.duplicated().sum()


final_data.tail()


final_data.dtypes


final_data['year'] = pd.to_datetime(final_data['year'].apply(lambda x: x.split('-')[-1])).dt.year


final_data.dtypes


final_data['State'].unique()





for i in final_data[final_data['crop'] == 'Rice']['State'].unique():

    rice_years = final_data[(final_data['State'] == i) & (final_data['crop'] == 'Rice')]['year']
    rice_volume = final_data[(final_data['State'] == i) & (final_data['crop'] == 'Rice')]['Volume']
    
    plt.figure(figsize=(10, 6))
    plt.plot(rice_years, rice_volume, marker='o', linestyle='-')
    plt.title(f'Volume of Rice in {i} Over the Years')
    plt.xlabel('Year')
    plt.ylabel('Volume (in tons)')
    plt.grid(True)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()





final_data['State'].unique()


import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))  # Create a single figure outside the loop

for state in final_data[final_data['crop'] == 'Rice']['State'].unique():
    rice_years = final_data[(final_data['State'] == state) & (final_data['crop'] == 'Rice')]['year']
    rice_volume = final_data[(final_data['State'] == state) & (final_data['crop'] == 'Rice')]['Volume']
    
    plt.plot(rice_years, rice_volume, marker='o', linestyle='-', label=state)

plt.title('Volume of Rice in Different States Over the Years')
plt.xlabel('Year')
plt.ylabel('Volume (in tons)')
plt.grid(True)
plt.xticks(rotation=45)
plt.legend(title='State', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()









for i in final_data[final_data['crop'] == 'Wheat']['State'].unique():

    wheat_years = final_data[(final_data['State'] == i) & (final_data['crop'] == 'Wheat')]['year']
    wheat_volume = final_data[(final_data['State'] == i) & (final_data['crop'] == 'Wheat')]['Volume']
    
    plt.figure(figsize=(10, 6))
    plt.plot(wheat_years, wheat_volume, marker='o', linestyle='-')
    plt.title(f'Volume of wheat in {i} Over the Years')
    plt.xlabel('Year')
    plt.ylabel('Volume (in tons)')
    plt.grid(True)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()


import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))  # Create a single figure outside the loop

for state in  final_data[final_data['crop'] == 'Wheat']['State'].unique():
    wheat_years = final_data[(final_data['State'] == state) & (final_data['crop'] == 'Wheat')]['year']
    wheat_volume = final_data[(final_data['State'] == state) & (final_data['crop'] == 'Wheat')]['Volume']
    
    plt.plot(wheat_years, wheat_volume, marker='o', linestyle='-', label=state)

plt.title('Volume of wheat in Different States Over the Years')
plt.xlabel('Year')
plt.ylabel('Volume (in tons)')
plt.grid(True)
plt.xticks(rotation=45)
plt.legend(title='State', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()






final_data['crop'].unique()


for i in final_data[final_data['crop'] == 'Coarse']['State'].unique():

    coarse_years = final_data[(final_data['State'] == i) & (final_data['crop'] == 'Coarse')]['year']
    coarse_volume = final_data[(final_data['State'] == i) & (final_data['crop'] == 'Coarse')]['Volume']
    
    plt.figure(figsize=(10, 6))
    plt.plot(coarse_years, coarse_volume, marker='o', linestyle='-')
    plt.title(f'Volume of coarse in {i} Over the Years')
    plt.xlabel('Year')
    plt.ylabel('Volume (in tons)')
    plt.grid(True)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()








import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))  # Create a single figure outside the loop

for state in  final_data[final_data['crop'] == 'Coarse']['State'].unique():
    Coarse_years = final_data[(final_data['State'] == state) & (final_data['crop'] == 'Coarse')]['year']
    Coarse_volume = final_data[(final_data['State'] == state) & (final_data['crop'] == 'Coarse')]['Volume']
    
    plt.plot(Coarse_years, Coarse_volume, marker='o', linestyle='-', label=state)

plt.title('Volume of wheat in Different States Over the Years')
plt.xlabel('Year')
plt.ylabel('Volume (in tons)')
plt.grid(True)
plt.xticks(rotation=45)
plt.legend(title='State', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()






final_data.sample(10)


sns.scatterplot(data=final_data,x='Area', y='Volume',hue='crop')
plt.xlabel('Area')
plt.ylabel('Volume')
plt.title('ScatterPlot ----Area X Volume')
plt.show()



numeric_cols = ['Area', 'Volume']

final_data[numeric_cols].corr()


Rice_data = final_data[final_data['crop'] == 'Rice']


Rice_data.shape


Rice_data


plt.figure(figsize=(10,10))
plt.bar(Rice_data['State'], Rice_data['Volume'])
plt.xlabel('State')
plt.ylabel('Volume')
plt.title('Rice')
plt.xticks(rotation=90)
plt.show()



plt.figure(figsize=(10,10))
plt.bar(Rice_data['year'], Rice_data['Volume'])
plt.xlabel('Year')
plt.ylabel('Volume')
plt.title('Rice')
plt.xticks(rotation=90)
plt.show()



Wheat_data = final_data[final_data['crop'] == 'Wheat']


Wheat_data.shape


plt.figure(figsize=(10,10))
plt.bar(Wheat_data['State'], Wheat_data['Volume'])
plt.xlabel('State')
plt.ylabel('Volume')
plt.title('Wheat')
plt.xticks(rotation=90)
plt.show()



plt.figure(figsize=(10,10))
plt.bar(Wheat_data['year'], Wheat_data['Volume'])
plt.xlabel('Year')
plt.ylabel('Volume')
plt.title('Wheat')
plt.xticks(rotation=90)
plt.show()



Coarse_data = final_data[final_data['crop'] == 'Coarse']


Coarse_data.shape


plt.figure(figsize=(10,10))
plt.bar(Coarse_data['State'], Coarse_data['Volume'])
plt.xlabel('State')
plt.ylabel('Volume')
plt.title('Coarse')
plt.xticks(rotation=90)
plt.show()



plt.figure(figsize=(10,10))
plt.bar(Coarse_data['year'], Coarse_data['Volume'])
plt.xlabel('Year')
plt.ylabel('Volume')
plt.title('Coarse')
plt.xticks(rotation=90)
plt.show()



final_data.head()





Rice_data.head()


grouped_data = Rice_data.groupby('State')
total_production_Rice_by_state = grouped_data['Volume'].sum()
total_production_Rice_by_state


plt.figure(figsize=(10, 10))
plt.pie(total_production_Rice_by_state, labels=total_production_Rice_by_state.index, autopct='%1.1f%%', startangle=140)
plt.title('Rice Production by State')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.show()





Wheat_data.head()


grouped_data = Wheat_data.groupby('State')
total_production_Wheat_by_state = grouped_data['Volume'].sum()
total_production_Wheat_by_state


plt.figure(figsize=(10, 10))
plt.pie(total_production_Wheat_by_state, labels=total_production_Wheat_by_state.index, autopct='%1.1f%%', startangle=140)
plt.gca().legend(total_production_Wheat_by_state.index, loc='center left', bbox_to_anchor=(1, 0.5), fontsize ='small', title='State', title_fontsize='medium')
plt.title('Wheat Production by State')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.show()





Coarse_data.head()


grouped_data = Coarse_data.groupby('State')
total_production_Coarse_by_state = grouped_data['Volume'].sum()
total_production_Coarse_by_state


plt.figure(figsize=(10, 10))
plt.pie(total_production_Coarse_by_state, labels=total_production_Coarse_by_state.index, autopct='%1.1f%%', startangle=140)
plt.gca().legend(total_production_Coarse_by_state.index, loc='center left', bbox_to_anchor=(1, 0.5), fontsize ='small', title='State', title_fontsize='medium')
plt.title('Coarse Production by State')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.show()





final_data.head()


plt.figure(figsize=(8, 6))
plt.hist(final_data['Area'], bins=10, color='skyblue', edgecolor='black')  # Adjust the number of bins as needed
plt.title('Area Histogram')
plt.xlabel('Area')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()


final_data.sample(10)


Wheat_data.sample()


plt.figure(figsize=(8, 6))
sns.boxplot(data=Rice_data['Area'].reset_index(drop=True), color='skyblue')
plt.title('Boxplot of Data')
plt.xlabel('Data')
plt.show()


plt.figure(figsize=(8, 6))
sns.boxplot(data=Wheat_data['Area'].reset_index(drop=True), color='skyblue')
plt.title('Boxplot of Data')
plt.xlabel('Data')
plt.show()


plt.figure(figsize=(8, 6))
sns.boxplot(data=Coarse_data['Area'].reset_index(drop=True), color='skyblue')
plt.title('Boxplot of Data')
plt.xlabel('Data')
plt.show()





sns.histplot(final_data['Area'])


## (1)<1 , <4 ,<8,<10 


final_data.shape


final_data.head()


final_data[(final_data['Area']>=8) & (final_data['Area']<=10) ].shape


def areasize(x):
    if x>=0 and x<1: #Area with lowest area
        return 1
    elif x>=1 and x<=2:  #Area with medium area
        return 2
    elif x>2 and x<=4:  #Area with medium area
        return 3
    elif x>4 and x<8: #Area with medium high area
        return 4
    elif x>=8 and x<=10: #Area with highest  area
        return 5


final_data['Areasize'] = final_data['Area'].apply(areasize) # making a new column on basis of size kind of ordinal encoding





final_data.head()





"""
# Check for outliers (consider using techniques like IQR - Interquartile Range)
# Example: Identify outliers in 'Volume' using IQR
Q1 = final_data['Area'].quantile(0.25)
Q3 = final_data['Area'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - (1.5 * IQR)
upper_bound = Q3 + (1.5 * IQR)
area_outliers = final_data[(final_data['Area'] < lower_bound) | (final_data['Area'] > upper_bound)]
# Decide how to handle outliers (keep, winsorize - cap to specific range, remove)
# Example: Print outliers for Volume
print("Potential outliers in Area:")
print(area_outliers.index)
"""

import numpy as np

# Calculate Z-scores for 'Area' column
z_scores = (final_data['Area'] - final_data['Area'].mean()) / final_data['Area'].std()

# Define threshold for outlier detection (e.g., Z-score > 3)
threshold = 1

# Identify outliers based on Z-scores
area_outliers = final_data[np.abs(z_scores) > threshold]

# Print outliers
print("Potential outliers in Area (Z-score method):")
print(area_outliers.index)




final_data.loc[area_outliers.index].head()


final_data.drop(area_outliers.index, inplace=True)


final_data.shape


"""
# Check for outliers (consider using techniques like IQR - Interquartile Range)
# Example: Identify outliers in 'Volume' using IQR
Q1 = final_data['Volume'].quantile(0.25)
Q3 = final_data['Volume'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - (1.5 * IQR)
upper_bound = Q3 + (1.5 * IQR)
volume_outliers = final_data[(final_data['Volume'] < lower_bound) | (final_data['Volume'] > upper_bound)]

# Decide how to handle outliers (keep, winsorize - cap to specific range, remove)
# Example: Print outliers for Volume
print("Potential outliers in Volume:")
print(volume_outliers.index)
"""

import numpy as np

# Calculate Z-scores for 'Volume' column
z_scores_volume = (final_data['Volume'] - final_data['Volume'].mean()) / final_data['Volume'].std()

# Define threshold for outlier detection (e.g., Z-score > 3)
threshold_volume = 1

# Identify outliers based on Z-scores
volume_outliers = final_data[np.abs(z_scores_volume) > threshold_volume]

# Print outliers
print("Potential outliers in Volume (Z-score method):")
print(volume_outliers.index)



final_data.loc[volume_outliers.index].head()


final_data.drop(volume_outliers.index, inplace=True)


final_data.shape


final_data.reset_index(inplace=True)


final_data.drop('index',inplace=True,axis=1)


data = final_data.drop(['Frequency', 'Unit'],axis=1)


data.head()


data.iloc[:,:-1].head()





from sklearn.model_selection import train_test_split


X =  data[['year', 'Area', 'State', 'crop', 'Areasize']]


X.head()


y = data['Volume']





X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=42,test_size=0.2)


X_train.shape














from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder

# Separate features into numerical and categorical
numerical_features = ['year', 'Area', 'Areasize']
categorical_features = ['State', 'crop']

# Define preprocessing steps for numerical features
numerical_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

# Define preprocessing steps for categorical features
categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore',drop='first'))
])

# Combine preprocessing steps for numerical and categorical features
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Define the preprocessing pipeline
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor)
])




"""
# Separate features into numerical
numerical_features = ['Area']

# Define preprocessing steps for numerical features
numerical_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

# Combine preprocessing steps for numerical features
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features)
    ])

# Define the preprocessing pipeline
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor)
])
"""








# Apply the preprocessing pipeline to the data
X_train_processed = pipeline.fit_transform(X_train)
X_test_processed = pipeline.transform(X_test)



from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Initialize the linear regression model
model = LinearRegression()

# Train the model on the preprocessed training data
model.fit(X_train_processed, y_train)

# Predict on the preprocessed testing data
y_pred = model.predict(X_test_processed)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Print evaluation metrics
print("Mean Squared Error (MSE):", mse)
print("Mean Absolute Error (MAE):", mae)
print("R-squared (R2):", r2)












from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Initialize the decision tree regressor model
decision_tree_model = DecisionTreeRegressor()

# Train the model on the preprocessed training data
decision_tree_model.fit(X_train_processed, y_train)

# Predict on the preprocessed testing data
y_pred = decision_tree_model.predict(X_test_processed)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Print evaluation metrics
print("Mean Squared Error (MSE):", mse)
print("Mean Absolute Error (MAE):", mae)
print("R-squared (R2):", r2)



from sklearn.ensemble import RandomForestRegressor

# Initialize the random forest regressor model
random_forest_model = RandomForestRegressor()

# Train the model on the preprocessed training data
random_forest_model.fit(X_train_processed, y_train)

# Predict on the preprocessed testing data
y_pred_random_forest = random_forest_model.predict(X_test_processed)

# Evaluate the model
mse_random_forest = mean_squared_error(y_test, y_pred_random_forest)
mae_random_forest = mean_absolute_error(y_test, y_pred_random_forest)
r2_random_forest = r2_score(y_test, y_pred_random_forest)

# Print evaluation metrics for Random Forest Regression
print("Random Forest Regression:")
print("Mean Squared Error (MSE):", mse_random_forest)
print("Mean Absolute Error (MAE):", mae_random_forest)
print("R-squared (R2):", r2_random_forest)



from sklearn.ensemble import GradientBoostingRegressor

# Initialize the gradient boosting regressor model
gradient_boosting_model = GradientBoostingRegressor()

# Train the model on the preprocessed training data
gradient_boosting_model.fit(X_train_processed, y_train)

# Predict on the preprocessed testing data
y_pred_gradient_boosting = gradient_boosting_model.predict(X_test_processed)

# Evaluate the model
mse_gradient_boosting = mean_squared_error(y_test, y_pred_gradient_boosting)
mae_gradient_boosting = mean_absolute_error(y_test, y_pred_gradient_boosting)
r2_gradient_boosting = r2_score(y_test, y_pred_gradient_boosting)

# Print evaluation metrics for Gradient Boosting Regression
print("Gradient Boosting Regression:")
print("Mean Squared Error (MSE):", mse_gradient_boosting)
print("Mean Absolute Error (MAE):", mae_gradient_boosting)
print("R-squared (R2):", r2_gradient_boosting)



from sklearn.svm import SVR

# Initialize the SVR model
svr_model = SVR()

# Train the model on the preprocessed training data
svr_model.fit(X_train_processed, y_train)

# Predict on the preprocessed testing data
y_pred_svr = svr_model.predict(X_test_processed)

# Evaluate the model
mse_svr = mean_squared_error(y_test, y_pred_svr)
mae_svr = mean_absolute_error(y_test, y_pred_svr)
r2_svr = r2_score(y_test, y_pred_svr)

# Print evaluation metrics for SVR
print("Support Vector Regression (SVR):")
print("Mean Squared Error (MSE):", mse_svr)
print("Mean Absolute Error (MAE):", mae_svr)
print("R-squared (R2):", r2_svr)



from sklearn.neural_network import MLPRegressor

# Initialize the MLP regressor model
mlp_model = MLPRegressor()

# Train the model on the preprocessed training data
mlp_model.fit(X_train_processed, y_train)

# Predict on the preprocessed testing data
y_pred_mlp = mlp_model.predict(X_test_processed)

# Evaluate the model
mse_mlp = mean_squared_error(y_test, y_pred_mlp)
mae_mlp = mean_absolute_error(y_test, y_pred_mlp)
r2_mlp = r2_score(y_test, y_pred_mlp)

# Print evaluation metrics for MLP Regression
print("Neural Network Regression (MLP):")
print("Mean Squared Error (MSE):", mse_mlp)
print("Mean Absolute Error (MAE):", mae_mlp)
print("R-squared (R2):", r2_mlp)



from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Initialize the KNN regressor model
knn_model = KNeighborsRegressor()

# Train the model on the preprocessed training data
knn_model.fit(X_train_processed, y_train)

# Predict on the preprocessed testing data
y_pred_knn = knn_model.predict(X_test_processed)

# Evaluate the model
mse_knn = mean_squared_error(y_test, y_pred_knn)
mae_knn = mean_absolute_error(y_test, y_pred_knn)
r2_knn = r2_score(y_test, y_pred_knn)

# Print evaluation metrics for KNN Regression
print("K-Nearest Neighbors Regression (KNN):")
print("Mean Squared Error (MSE):", mse_knn)
print("Mean Absolute Error (MAE):", mae_knn)
print("R-squared (R2):", r2_knn)






from sklearn.model_selection import cross_val_score
import pandas as pd
from sklearn.linear_model import LinearRegression,Ridge,Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

models = [
    LinearRegression(),
    Ridge(),
    Lasso(),
    DecisionTreeRegressor(),
    RandomForestRegressor(),
    GradientBoostingRegressor(),
    SVR(),
    MLPRegressor(max_iter=2000),
    KNeighborsRegressor()
]

results = []

X_processed = pipeline.fit_transform(X)
for model in models:
    # Perform cross-validation
    cv_scores_mse = cross_val_score(model, X_processed, y, cv=3, scoring='neg_mean_squared_error')
    cv_scores_mae = cross_val_score(model, X_processed, y, cv=3, scoring='neg_mean_absolute_error')
    cv_scores_r2 = cross_val_score(model, X_processed, y, cv=3, scoring='r2')
    
    # Calculate the mean scores across folds
    mse_mean = -cv_scores_mse.mean()
    mae_mean = -cv_scores_mae.mean()
    r2_mean = cv_scores_r2.mean()
    
    results.append({
        'Model': type(model).__name__,
        'Mean Squared Error (MSE)': mse_mean,
        'Mean Absolute Error (MAE)': mae_mean,
        'R-squared (R2)': r2_mean
    })


# Create a DataFrame from the results list
results_df = pd.DataFrame(results)

# Display the DataFrame
results_df





from sklearn.model_selection import GridSearchCV

# Define hyperparameters grid for Ridge regression
ridge_params = {'alpha': [0.1, 1.0, 10.0]}

# Define hyperparameters grid for Lasso regression
lasso_params = {'alpha': [0.1, 1.0, 10.0]}

# Define hyperparameters grid for Decision Tree
tree_params = {'max_depth': [None, 10, 20],
               'min_samples_split': [2, 5, 10],
               'min_samples_leaf': [1, 2, 4]}

# Define hyperparameters grid for Random Forest
forest_params = {'n_estimators': [100, 200, 300],
                 'max_depth': [None, 10, 20],
                 'min_samples_split': [2, 5, 10],
                 'min_samples_leaf': [1, 2, 4]}

# Define hyperparameters grid for Gradient Boosting
gb_params = {'n_estimators': [100, 200, 300],
             'learning_rate': [0.01, 0.1, 0.5],
             'max_depth': [3, 5, 10],
             'min_samples_split': [2, 5, 10],
             'min_samples_leaf': [1, 2, 4]}

# Define hyperparameters grid for SVM
svm_params = {'C': [0.1, 1.0, 10.0],
              'kernel': ['linear', 'rbf', 'poly']}

# Define hyperparameters grid for MLPRegressor
mlp_params = {'hidden_layer_sizes': [(50,), (100,), (50, 50)],
              'activation': ['relu', 'tanh', 'logistic'],
              'alpha': [0.0001, 0.001, 0.01],
              'max_iter': [2000],  # Increase max_iter
              'learning_rate_init': [0.001, 0.01, 0.1]}  # Adjust learning rate


# Define hyperparameters grid for KNeighborsRegressor
knn_params = {'n_neighbors': [2,3,4, 5,6, 7],
              'weights': ['uniform', 'distance'],
              'p': [1, 2]}

# Create a dictionary mapping each model to its respective hyperparameters grid
param_grids = {
    Ridge(): ridge_params,
    Lasso(): lasso_params,
    DecisionTreeRegressor(): tree_params,
    RandomForestRegressor(): forest_params,
    GradientBoostingRegressor(): gb_params,
    SVR(): svm_params,
    MLPRegressor(max_iter=2000): mlp_params,
    KNeighborsRegressor(): knn_params
}

results_hyper = []

for model, param_grid in param_grids.items():
    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
    grid_search.fit(X_processed, y)
    
    best_params = grid_search.best_params_
    best_mse = -grid_search.best_score_
    
    # Perform cross-validation with the best model to compute R2 score
    best_model = grid_search.best_estimator_
    cv_scores_r2 = cross_val_score(best_model, X_processed, y, cv=5, scoring='r2')
    best_r2 = cv_scores_r2.mean()
    
    results_hyper.append({
        'Model': type(model).__name__,
        'Best Parameters': best_params,
        'Best Mean Squared Error': best_mse,
        'Mean R-squared (R2)': best_r2
    })

# Create a DataFrame from the hyperparameter tuning results
results_hyper_df = pd.DataFrame(results_hyper)

# Display the DataFrame
results_hyper_df









